{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 1**"
      ],
      "metadata": {
        "id": "-NMFAj_lb10p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regression (GBR) is a machine learning algorithm used for regression problems, which is an extension of the popular Gradient Boosting algorithm. GBR is a type of ensemble learning method that combines multiple decision trees to improve the accuracy of predictions.\n",
        "\n",
        "In GBR, a series of decision trees are built sequentially, where each new tree is trained to predict the residual errors of the previous tree. The final prediction is the sum of the predictions made by all the trees in the sequence. The term \"gradient\" in the name of the algorithm refers to the use of gradient descent optimization to minimize the loss function at each step of the learning process.\n",
        "\n",
        "GBR is a powerful algorithm that can handle complex non-linear relationships between the input features and the output variable. It is often used in applications such as stock price prediction, customer churn prediction, and disease diagnosis. However, it requires careful tuning of hyperparameters and can be computationally expensive, especially for large datasets."
      ],
      "metadata": {
        "id": "vgAlV12rb3iG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 2**"
      ],
      "metadata": {
        "id": "9ESX8sFudM9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split , GridSearchCV\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#Load the dataset\n",
        "X,y = load_diabetes(return_X_y=True)\n",
        "\n",
        "#Train test split\n",
        "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state= 42)\n",
        "\n",
        "#define parameter grid\n",
        "param_grid = { 'learning_rate': [0.1,1,10,100],'n_estimators':[50,200,100],'max_depth':[3,4,2]}\n",
        "\n",
        "#define estimator\n",
        "gb= GradientBoostingRegressor()\n",
        "\n",
        "#cv\n",
        "grid_search= GridSearchCV(estimator=gb,param_grid= param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "#fit\n",
        "grid_search.fit(X_train,y_train)\n",
        "\n",
        "#extract the best model\n",
        "best_model= grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Compute the mean squared error on the testing data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"R-squared: \", r2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5awSU9q_7zLP",
        "outputId": "0f125f99-147e-4cd5-d2f6-30d3343e188c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
            "MSE: 2700.1676950593965\n",
            "R-squared:  0.49035667092483703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ 3.90274512e-001  3.20581667e-001  3.61644836e-001  3.51971716e-001\n",
            "  3.00244136e-001  3.23629370e-001  4.07597361e-001  3.56704996e-001\n",
            "  3.90764818e-001 -2.00622750e-001 -2.00326798e-001 -2.29443009e-001\n",
            " -5.85815003e-002 -9.65568247e-002 -8.82617519e-002 -2.18663638e-002\n",
            " -1.08017911e-001 -6.36866320e-002 -1.46014495e+095             -inf\n",
            " -3.86957667e+190 -1.56399596e+095             -inf -4.15027209e+190\n",
            " -1.30047054e+095             -inf -3.45423167e+190 -2.01633099e+199\n",
            "              nan             -inf -2.28057096e+199              nan\n",
            "             -inf -1.79212804e+199              nan             -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: overflow encountered in square\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 3**"
      ],
      "metadata": {
        "id": "PkcCxxq7dRTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split , GridSearchCV\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#Load the dataset\n",
        "X,y = load_diabetes(return_X_y=True)\n",
        "\n",
        "#Train test split\n",
        "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state= 42)\n",
        "\n",
        "#define parameter grid\n",
        "param_grid = { 'learning_rate': [0.1,1,10,100],'n_estimators':[50,200,100],'max_depth':[3,4,2]}\n",
        "\n",
        "#define estimator\n",
        "gb= GradientBoostingRegressor()\n",
        "\n",
        "#cv\n",
        "grid_search= GridSearchCV(estimator=gb,param_grid= param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "#fit\n",
        "grid_search.fit(X_train,y_train)\n",
        "\n",
        "#extract the best model\n",
        "best_model= grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Compute the mean squared error on the testing data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"R-squared: \", r2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPlwTrxnspx-",
        "outputId": "3eaf4c0e-2cec-4a35-8fb1-346ee73e6dc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
            "MSE: 2714.9909988641793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ 3.88901313e-001  3.19380077e-001  3.61780326e-001  3.54433351e-001\n",
            "  2.97151592e-001  3.30328234e-001  4.07177942e-001  3.55572888e-001\n",
            "  3.90154279e-001 -2.10120180e-001 -2.63444178e-001 -2.21832307e-001\n",
            " -1.45520945e-001 -3.96333058e-002 -7.46107501e-002 -5.83666767e-003\n",
            " -1.05702484e-001 -9.86637935e-002 -1.46014495e+095             -inf\n",
            " -3.86947648e+190 -1.56430212e+095             -inf -4.15365397e+190\n",
            " -1.30047054e+095             -inf -3.45423167e+190 -2.01593603e+199\n",
            "              nan             -inf -2.27785228e+199              nan\n",
            "             -inf -1.79212804e+199              nan             -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: overflow encountered in square\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 4**"
      ],
      "metadata": {
        "id": "6mntCFMozAuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In gradient boosting, a weak learner is a model that performs slightly better than random guessing, but is not sufficiently powerful to solve the problem at hand on its own.\n",
        "\n",
        "Gradient boosting works by iteratively adding weak learners to the model to improve its overall performance. At each iteration, the algorithm fits a weak learner to the residuals (the difference between the predicted values and the actual values) of the previous iteration, and adds the output of the weak learner to the ensemble of models.\n",
        "\n",
        "The key idea behind using weak learners in gradient boosting is that, although individual weak learners are not very powerful, their collective performance can be significantly boosted when they are combined in an ensemble. By iteratively adding weak learners, the model gradually learns to fit the data more accurately, ultimately achieving high performance on the problem at hand."
      ],
      "metadata": {
        "id": "OFEX-4dn4s5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 5**"
      ],
      "metadata": {
        "id": "YMHzAXWL4v58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting is a machine learning algorithm that is used for both regression and classification problems. It belongs to the family of boosting algorithms, which iteratively combine weak learners into a strong one. The intuition behind the Gradient Boosting algorithm is to build a model that can predict the target variable by combining the predictions of several weak models.\n",
        "\n",
        "In Gradient Boosting, the weak models are typically decision trees with a small number of nodes. The algorithm works by fitting a decision tree to the training data and then using the residuals of the predicted values to fit a second decision tree. This process is repeated until a specified number of trees are constructed or until a certain threshold is reached in the reduction of the residuals.\n",
        "\n",
        "At each iteration, the algorithm calculates the gradient of the loss function with respect to the predicted values and uses this gradient to fit the next decision tree. The gradient represents the direction in which the algorithm needs to update the predicted values in order to reduce the loss function. By iteratively adding weak learners and updating the predicted values, the algorithm gradually improves the accuracy of the model.\n",
        "\n",
        "One of the key advantages of Gradient Boosting is that it can handle a variety of loss functions, such as squared loss for regression problems and logistic loss for classification problems. It can also handle missing data and outliers, and is relatively robust to overfitting. However, Gradient Boosting can be sensitive to the choice of hyperparameters, such as the learning rate and the number of trees, and may require extensive tuning to achieve optimal performance."
      ],
      "metadata": {
        "id": "WMHYBOw94xJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 6**"
      ],
      "metadata": {
        "id": "iW9KejxI5Uwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting builds an ensemble of weak learners by iteratively adding decision trees to the model. The algorithm starts by fitting a simple model to the data, such as a decision tree with a small number of nodes. This model is referred to as the first weak learner.\n",
        "\n",
        "Once the first weak learner is constructed, the algorithm evaluates its performance on the training data and calculates the residuals, which are the differences between the predicted and actual values of the target variable. The residuals are then used as the new target variable for the next weak learner.\n",
        "\n",
        "The second weak learner is constructed by fitting another decision tree to the residuals of the first weak learner. This tree is designed to capture the patterns in the residuals that were not captured by the first tree. The predicted values of the second weak learner are then added to the predicted values of the first weak learner to produce an updated prediction.\n",
        "\n",
        "This process is repeated, with each new weak learner being fitted to the residuals of the previous ensemble. The predicted values of the new learner are added to the predicted values of the previous ensemble to produce an updated prediction. The number of iterations is determined by a parameter called the \"number of trees\", which is a hyperparameter that is typically tuned using cross-validation.\n",
        "\n",
        "The final prediction of the Gradient Boosting model is the sum of the predicted values of all the weak learners in the ensemble. Each weak learner is assigned a weight that determines its contribution to the final prediction. The weights are typically learned using a gradient descent algorithm that minimizes a loss function, such as mean squared error or cross-entropy. The gradient descent algorithm updates the weights in a way that minimizes the error of the entire ensemble, making the final prediction more accurate than any individual weak learner."
      ],
      "metadata": {
        "id": "If6KhMr25ZNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 7**"
      ],
      "metadata": {
        "id": "3BMQtK3p59t0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical intuition behind Gradient Boosting can be broken down into the following steps:\n",
        "\n",
        "Define a loss function: The first step in constructing the mathematical intuition of Gradient Boosting is to define a loss function. This function measures the difference between the predicted and actual values of the target variable. For regression problems, the most commonly used loss function is mean squared error (MSE), while for classification problems, cross-entropy is often used.\n",
        "\n",
        "Fit a simple model: The next step is to fit a simple model, such as a decision tree, to the data. This model is often referred to as the first weak learner.\n",
        "\n",
        "Evaluate the model: After fitting the first weak learner, the model's performance is evaluated by computing the loss function on the training data.\n",
        "\n",
        "Compute the residuals: The residuals are the differences between the predicted and actual values of the target variable. The residuals are used as the new target variable for the next weak learner.\n",
        "\n",
        "Fit a new weak learner: A new weak learner is then fit to the residuals. This weak learner is designed to capture the patterns in the residuals that were not captured by the previous weak learner.\n",
        "\n",
        "Update the predicted values: The predicted values of the new weak learner are added to the predicted values of the previous ensemble to produce an updated prediction.\n",
        "\n",
        "Repeat steps 4-6: Steps 4-6 are repeated until a specified number of trees are constructed or until a certain threshold is reached in the reduction of the residuals.\n",
        "\n",
        "Assign weights: Each weak learner in the ensemble is assigned a weight that determines its contribution to the final prediction. The weights are learned using a gradient descent algorithm that minimizes the loss function.\n",
        "\n",
        "Combine the weak learners: The final prediction of the Gradient Boosting model is the sum of the predicted values of all the weak learners in the ensemble, weighted by their contribution.\n",
        "\n",
        "Tune hyperparameters: The performance of the Gradient Boosting model depends on the choice of hyperparameters, such as the learning rate and the number of trees. These hyperparameters can be tuned using cross-validation to achieve optimal performance."
      ],
      "metadata": {
        "id": "27Pab2Lp5_qz"
      }
    }
  ]
}