{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ques1**"
      ],
      "metadata": {
        "id": "d0oeWC43CCff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem is a mathematical formula that describes the probability of an event based on prior knowledge of related events. It is named after the Reverend Thomas Bayes, an 18th-century English statistician and theologian who first formulated the idea.\n",
        "\n",
        "Bayes' theorem states that the probability of an event A given that event B has occurred is equal to the probability of event B given that event A has occurred, multiplied by the probability of event A, and divided by the probability of event B. Mathematically, it can be expressed as:\n",
        "\n",
        "P(A|B) = P(B|A) * P(A) / P(B)\n",
        "\n",
        "Where:\n",
        "\n",
        "P(A|B) is the probability of event A given event B has occurred.\n",
        "\n",
        "P(B|A) is the probability of event B given that event A has occurred.\n",
        "\n",
        "P(A) is the prior probability of event A occurring.\n",
        "\n",
        "P(B) is the prior probability of event B occurring.\n",
        "\n",
        "Bayes' theorem is widely used in statistics, data science, machine learning, and artificial intelligence to make predictions and decisions based on available data and prior knowledge. It is especially useful in situations where there is uncertainty or incomplete information."
      ],
      "metadata": {
        "id": "eBsmkl6NCESP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 2**"
      ],
      "metadata": {
        "id": "4xhj4vgtFQQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem is a fundamental concept in probability theory and statistics that describes the relationship between the conditional probabilities of two events. The formula for Bayes' theorem is:\n",
        "\n",
        "P(A|B) = P(B|A) * P(A) / P(B)\n",
        "\n",
        "where:\n",
        "\n",
        "P(A|B) is the probability of event A occurring given that event B has occurred (known as the posterior probability).\n",
        "\n",
        "P(B|A) is the probability of event B occurring given that event A has occurred (known as the likelihood).\n",
        "\n",
        "P(A) is the prior probability of event A occurring.\n",
        "\n",
        "P(B) is the prior probability of event B occurring.\n",
        "\n",
        "In other words, Bayes' theorem allows us to update our beliefs about the probability of an event A occurring given new information in the form of event B. We start with an initial estimate of the probability of A (the prior probability) and then update this estimate based on the likelihood of B given A and the overall probability of B occurring."
      ],
      "metadata": {
        "id": "HMlena1QFSYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 3**"
      ],
      "metadata": {
        "id": "1QPH6iyxFliS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem is used in a wide variety of fields and applications, including science, engineering, medicine, economics, and more. Here are a few examples of how Bayes' theorem is used in practice:\n",
        "\n",
        "Medical diagnosis: Bayes' theorem is used in medical diagnosis to help doctors estimate the probability of a patient having a particular disease given their symptoms, test results, and other factors. By updating the prior probability of a disease with new information, doctors can make more accurate diagnoses and develop more effective treatment plans.\n",
        "\n",
        "Spam filtering: Many email systems use Bayes' theorem to filter out spam messages from legitimate ones. By analyzing the content of an email and calculating the probability that it is spam or not, the system can automatically filter out unwanted messages and protect users from phishing attacks and other scams.\n",
        "\n",
        "Fault diagnosis: In engineering, Bayes' theorem is used for fault diagnosis in complex systems such as aircraft, vehicles, and machinery. By combining sensor data with prior knowledge about the system, engineers can identify faults and diagnose problems more quickly and accurately.\n",
        "\n",
        "Financial forecasting: Bayes' theorem is used in finance to forecast future market trends and estimate the probabilities of different investment outcomes. By updating prior beliefs with new market data and economic indicators, analysts can make more informed investment decisions and minimize risks.\n",
        "\n",
        "These are just a few examples of how Bayes' theorem is used in practice. Its versatility and wide applicability make it a powerful tool for decision-making and problem-solving in many different fields."
      ],
      "metadata": {
        "id": "LQ5Zfx93Fn0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 4**"
      ],
      "metadata": {
        "id": "nYoGYVbCGBLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem and conditional probability are closely related concepts in probability theory. Conditional probability is the probability of an event occurring given that another event has occurred, while Bayes' theorem describes how to update our prior beliefs about the probability of an event given new information in the form of another event.\n",
        "\n",
        "In fact, Bayes' theorem can be derived from the definition of conditional probability. If we have two events A and B, then the conditional probability of A given B can be expressed as:\n",
        "\n",
        "P(A|B) = P(A ∩ B) / P(B)\n",
        "\n",
        "where P(A ∩ B) is the probability of both A and B occurring. We can then rewrite this equation as:\n",
        "\n",
        "P(A ∩ B) = P(B|A) * P(A)\n",
        "\n",
        "which states that the probability of both A and B occurring is equal to the probability of B given A times the probability of A. By substituting this into the previous equation, we get:\n",
        "\n",
        "P(A|B) = P(B|A) * P(A) / P(B)\n",
        "\n",
        "which is Bayes' theorem.\n",
        "\n",
        "So, in essence, Bayes' theorem provides a way to calculate the conditional probability of A given B by incorporating prior knowledge about the probability of A and the likelihood of B given A. This makes it a powerful tool for updating our beliefs in light of new information and making more informed decisions."
      ],
      "metadata": {
        "id": "jCLYBYDXGD6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 5**"
      ],
      "metadata": {
        "id": "Q9bxrP5BHIQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the appropriate type of Naive Bayes classifier depends on the nature of the data and the problem you are trying to solve. There are three common types of Naive Bayes classifiers:\n",
        "\n",
        "Gaussian Naive Bayes: This type of classifier is suitable for continuous data that follows a normal (Gaussian) distribution. It assumes that the features are independent and normally distributed, and uses the mean and standard deviation of each feature to estimate the probability density function.\n",
        "\n",
        "Multinomial Naive Bayes: This type of classifier is suitable for discrete data such as text data or word counts. It assumes that the features represent the frequency of occurrence of different words or terms, and uses the probabilities of each feature to estimate the likelihood of a particular class.\n",
        "\n",
        "Bernoulli Naive Bayes: This type of classifier is also suitable for discrete data such as text data or binary features. It assumes that each feature is binary (i.e., either present or absent), and uses the presence or absence of each feature to estimate the likelihood of a particular class.\n",
        "\n",
        "To choose the appropriate type of Naive Bayes classifier, you should consider the following factors:\n",
        "\n",
        "The type of data: Continuous or discrete?\n",
        "\n",
        "The distribution of the data: Does it follow a normal distribution?\n",
        "\n",
        "The nature of the features: Are they binary or count-based?\n",
        "\n",
        "The size of the dataset: Is the dataset large enough to support the assumptions of the model?\n",
        "\n",
        "It's also important to note that Naive Bayes classifiers work best when the features are independent, meaning that the presence or absence of one feature does not affect the likelihood of another feature. If the features are highly correlated, then other types of classifiers may be more appropriate.\n",
        "\n",
        "In summary, the choice of Naive Bayes classifier depends on the type of data, the distribution of the data, and the nature of the features. It's important to consider these factors carefully and choose the appropriate model for the problem at hand."
      ],
      "metadata": {
        "id": "pZ-weLSkHKlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 6**"
      ],
      "metadata": {
        "id": "TeLwuff5Huct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To classify the new instance with features X1=3 and X2=4 using Naive Bayes, we need to calculate the posterior probabilities for each class:\n",
        "\n",
        "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)\n",
        "\n",
        "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)\n",
        "\n",
        "Since we assume equal prior probabilities for each class (i.e., P(A) = P(B) = 0.5), we can simplify the calculations to:\n",
        "\n",
        "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) / P(X1=3,X2=4)\n",
        "\n",
        "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) / P(X1=3,X2=4)\n",
        "\n",
        "To calculate the likelihood probabilities P(X1=3,X2=4|A) and P(X1=3,X2=4|B), we can use the Naive Bayes assumption that the features are independent given the class. This means that we can calculate the probabilities of each feature separately and multiply them together:\n",
        "\n",
        "P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A)\n",
        "\n",
        "= 4/10 * 3/10\n",
        "\n",
        "= 0.12\n",
        "\n",
        "P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B)\n",
        "\n",
        "= 1/7 * 3/7\n",
        "\n",
        "= 0.0612\n",
        "\n",
        "To calculate the marginal probability of observing the feature values X1=3 and X2=4, we can sum over all the classes:\n",
        "\n",
        "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B)\n",
        "\n",
        "= 0.12 * 0.5 + 0.0612 * 0.5\n",
        "\n",
        "= 0.0906\n",
        "\n",
        "Now we can calculate the posterior probabilities:\n",
        "\n",
        "P(A|X1=3,X2=4) = 0.12 / 0.0906\n",
        "\n",
        "= 1.323\n",
        "\n",
        "P(B|X1=3,X2=4) = 0.0612 / 0.0906\n",
        "\n",
        "= 0.677\n",
        "\n",
        "Since P(A|X1=3,X2=4) > P(B|X1=3,X2=4), Naive Bayes would predict that the new instance belongs to class A."
      ],
      "metadata": {
        "id": "GGNlbYWbHxw-"
      }
    }
  ]
}