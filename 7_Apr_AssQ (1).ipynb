{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 1**"
      ],
      "metadata": {
        "id": "K6Km04ZLT-af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, kernel functions are used to transform input data into a higher-dimensional feature space, which may make it easier to separate the data with a linear decision boundary. Polynomial functions are one type of kernel function that can be used in kernel-based algorithms.\n",
        "\n",
        "A polynomial kernel is a type of kernel function that computes the dot product of two vectors in a feature space that corresponds to a polynomial of a certain degree. In other words, a polynomial kernel function is a way to compute the similarity between two vectors in a higher-dimensional space without explicitly computing the coordinates of that space. The degree of the polynomial is a hyperparameter that determines the complexity of the decision boundary.\n",
        "\n",
        "Polynomial functions are used as kernel functions because they are computationally efficient and can capture complex nonlinear relationships between features. In particular, polynomial kernels are useful for datasets with nonlinear decision boundaries that cannot be separated by a linear classifier.\n",
        "\n",
        "To summarize, polynomial functions are a specific type of kernel function that can be used to transform input data into a higher-dimensional feature space in order to capture complex nonlinear relationships between features and make it easier to separate the data with a linear decision boundary."
      ],
      "metadata": {
        "id": "nZx3j0EXUAAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques2**"
      ],
      "metadata": {
        "id": "E_a2tAXZXknD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the datasets\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "#Load data\n",
        "iris = load_iris()\n",
        "\n",
        "#Train test split\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.2,random_state=42)\n",
        "\n",
        "#define classifier\n",
        "svm = SVC(kernel='poly',degree = 3)\n",
        "\n",
        "#train\n",
        "svm.fit(X_train,y_train)\n",
        "\n",
        "#predict\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "#perf metrics\n",
        "accuracy= accuracy_score(y_test,y_pred)\n",
        "print(accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeTD00KgJFm3",
        "outputId": "fa4458a0-e2e4-4a4c-991c-731c7d47f608"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 3**"
      ],
      "metadata": {
        "id": "POSTYs2XX63b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Support Vector Regression (SVR), the value of epsilon (Îµ) is a hyperparameter that controls the width of the margin around the regression line. When the value of epsilon is increased, the margin becomes wider, allowing more training examples to be within the margin or even on the wrong side of the hyperplane, which can lead to more support vectors.\n",
        "\n",
        "More formally, a support vector is a training example that is used to define the regression line, meaning that its distance to the regression line is equal to epsilon. In other words, support vectors are the training examples that lie on the boundary of the margin.\n",
        "\n",
        "When the value of epsilon is increased, the margin becomes wider, and more training examples can fall within it, leading to more support vectors. This is because the wider margin allows the regression line to be more flexible, which means it can fit the training data better, including those examples that fall within the margin.\n",
        "\n",
        "However, increasing the value of epsilon too much can lead to overfitting, where the regression line becomes too complex and starts to fit the training data too closely, resulting in poor generalization performance on new data.\n",
        "\n",
        "In summary, increasing the value of epsilon can lead to an increase in the number of support vectors in SVR, but it is important to find an appropriate value that balances the trade-off between model complexity and generalization performance."
      ],
      "metadata": {
        "id": "GonE8CRiX_8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 4**"
      ],
      "metadata": {
        "id": "_7w2mwDSYVvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Regression (SVR) is a machine learning algorithm used for regression analysis. It uses a kernel function to transform the input data into a higher dimensional feature space where it can perform linear regression. The performance of the SVR is greatly affected by the choice of kernel function and the parameters C, epsilon, and gamma.\n",
        "\n",
        "Kernel Function:\n",
        "The kernel function plays a crucial role in SVR. It is used to map the input data to a higher dimensional feature space where linear regression can be performed. The choice of kernel function depends on the type of data and the problem at hand. Some commonly used kernel functions are:\n",
        "\n",
        "Linear kernel: It is the simplest kernel function and is used for linear regression.\n",
        "\n",
        "Polynomial kernel: It is used for non-linear regression problems.\n",
        "\n",
        "Radial basis function (RBF) kernel: It is used for non-linear regression problems and is the most commonly used kernel function.\n",
        "\n",
        "C Parameter:\n",
        "\n",
        "The C parameter is a regularization parameter that controls the trade-off between achieving a low training error and a low testing error. A high value of C means that the model will have a low training error but may overfit the data. A low value of C means that the model will have a high training error but may generalize better to new data. In general, it is recommended to start with a small value of C and gradually increase it if the model is underfitting. Conversely, if the model is overfitting, then the value of C should be decreased.\n",
        "\n",
        "Epsilon Parameter:\n",
        "\n",
        "The epsilon parameter is a tolerance parameter that controls the width of the epsilon-insensitive zone. It is used to ensure that the errors between the predicted values and the actual values are within a certain tolerance. A higher value of epsilon means that the model will be more tolerant of errors and will allow more points to fall within the epsilon-insensitive zone. Conversely, a lower value of epsilon means that the model will be less tolerant of errors and will only allow points within a narrow range of the predicted value.\n",
        "\n",
        "Gamma Parameter:\n",
        "\n",
        "The gamma parameter determines the shape of the decision boundary. A high value of gamma means that the decision boundary will be more complex and will fit the training data very closely, leading to overfitting. A low value of gamma means that the decision boundary will be smoother and will generalize better to new data, but may underfit the training data. In general, it is recommended to start with a small value of gamma and gradually increase it if the model is underfitting. Conversely, if the model is overfitting, then the value of gamma should be decreased.\n",
        "\n",
        "Here are some examples of when you might want to increase or decrease each parameter:\n",
        "\n",
        "Kernel Function:\n",
        "\n",
        "If the data is linearly separable, then a linear kernel should be used.\n",
        "\n",
        "If the data has non-linear patterns, then a polynomial or RBF kernel should be used.\n",
        "\n",
        "C Parameter:\n",
        "\n",
        "If the model is underfitting, then the value of C should be increased.\n",
        "\n",
        "If the model is overfitting, then the value of C should be decreased.\n",
        "\n",
        "Epsilon Parameter:\n",
        "\n",
        "If the model is underfitting, then the value of epsilon should be decreased to reduce the tolerance for errors.\n",
        "\n",
        "If the model is overfitting, then the value of epsilon should be increased to allow more points to fall within the epsilon-insensitive zone.\n",
        "\n",
        "Gamma Parameter:\n",
        "\n",
        "If the model is underfitting, then the value of gamma should be increased to make the decision boundary more complex.\n",
        "\n",
        "If the model is overfitting, then the value of gamma should be decreased to make the decision boundary smoother."
      ],
      "metadata": {
        "id": "1Pv3xqcyY379"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 5**"
      ],
      "metadata": {
        "id": "42PRzkXPZF8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "#Load dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X= breast_cancer.data\n",
        "y=breast_cancer.target\n",
        "\n",
        "#split\n",
        "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state= 42)\n",
        "\n",
        "#Preprocess data\n",
        "scaler= StandardScaler()\n",
        "X_train_scaled=scaler.fit_transform(X_train)\n",
        "X_test_scaled= scaler.transform(X_test)\n",
        "\n",
        "#define classifier\n",
        "svm= SVC()\n",
        "\n",
        "#train\n",
        "svm.fit(X_train_scaled,y_train)\n",
        "\n",
        "#pred\n",
        "y_pred= svm.predict(X_test_scaled)\n",
        "\n",
        "#Perf check\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "\n",
        "#tune the hyperparameters \n",
        "param_grid = {'C':[0.1,10,100] , 'gamma':[0.1,1,10], 'kernel': ['rbf','linear']}\n",
        "grid_search= GridSearchCV(SVC(),param_grid,cv=5)\n",
        "grid_search.fit(X_train_scaled,y_train)\n",
        "print(\"Best  params : \",grid_search.best_params_)\n",
        "\n",
        "#train tuned classifier on entire dataset\n",
        "best_svc=grid_search.best_estimator_\n",
        "X_scaled = scaler.transform(X)\n",
        "best_svc.fit(X_scaled, y)\n",
        "\n",
        "#pickling\n",
        "with open('breast_cancer_svc.pkl' , 'wb') as file:\n",
        "  pickle.dump(best_svc,file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al2XXFz0o4Gs",
        "outputId": "1127c18e-f666-4141-8ef4-ffaba45bcfee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9824561403508771\n",
            "Best  params :  {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n"
          ]
        }
      ]
    }
  ]
}