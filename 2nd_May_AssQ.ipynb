{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 1**"
      ],
      "metadata": {
        "id": "20o8E4TJvN3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection is a technique used to identify rare events or observations in a dataset that differ significantly from the majority of other observations. These rare events are often referred to as anomalies or outliers. Anomaly detection can be used in a wide range of applications, such as fraud detection, fault detection, intrusion detection, and quality control.\n",
        "\n",
        "The purpose of anomaly detection is to identify unusual patterns or outliers in a dataset that do not conform to the expected behavior or normal distribution of the data. Anomalies may be indicative of significant events or problems, such as fraud, system failures, or safety hazards. By identifying and isolating these anomalies, organizations can take appropriate action to mitigate the negative impact of these events, while also gaining insights into the underlying causes and patterns of anomalies in the data. Anomaly detection is thus an important tool for improving data quality, identifying potential risks, and enhancing decision-making processes."
      ],
      "metadata": {
        "id": "_SHlydfdvPlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 2**"
      ],
      "metadata": {
        "id": "p6lU3tY1vrfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection can be a challenging task due to several reasons. Some of the key challenges in anomaly detection are:\n",
        "\n",
        "Lack of labeled data: Anomaly detection often relies on labeled data to train machine learning models. However, in many real-world applications, labeled data is scarce or non-existent, making it difficult to develop accurate models.\n",
        "\n",
        "Imbalanced data: In many datasets, the number of anomalies is very small compared to the number of normal observations. This can make it challenging for machine learning algorithms to accurately detect anomalies, as they may be biased towards the majority class.\n",
        "\n",
        "Complex data patterns: Anomaly detection may be challenging when the data contains complex patterns that are difficult to identify. For example, anomalies may occur in multi-dimensional data, which can be difficult to visualize and analyze.\n",
        "\n",
        "Concept drift: Anomaly detection models may become less accurate over time if the underlying data patterns change or drift. This can happen, for example, if new types of anomalies emerge or if the normal behavior of the system changes.\n",
        "\n",
        "Computational complexity: Some anomaly detection techniques, such as deep learning models, may require significant computational resources and may be difficult to train and optimize.\n",
        "\n",
        "Interpretability: Finally, it can be challenging to interpret the results of anomaly detection models and understand the underlying causes of detected anomalies. This can make it difficult to take appropriate actions and improve the system over time."
      ],
      "metadata": {
        "id": "uaTHkG24vs_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 3**"
      ],
      "metadata": {
        "id": "-vZqa65YwM6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Unsupervised anomaly detection and supervised anomaly detection differ in the availability of labeled data.\n",
        "\n",
        "Supervised anomaly detection requires labeled data, where each data point is explicitly marked as either normal or anomalous. The model is trained on this labeled data to learn the patterns that distinguish normal data from anomalous data. During testing, the model predicts whether a new data point is normal or anomalous based on what it learned from the labeled training data.\n",
        "\n",
        "On the other hand, unsupervised anomaly detection does not require labeled data. The model is trained on a dataset with no explicit labels, and it learns to identify anomalies by identifying data points that deviate significantly from the majority of the data. This approach assumes that anomalies are rare events, so it tries to learn the distribution of the majority of the data and flags data points that have low probability under that distribution as anomalies.\n",
        "\n",
        "Unsupervised anomaly detection can be useful when labeled data is scarce or difficult/expensive to obtain, while supervised anomaly detection can be more effective when labeled data is available and there is a clear distinction between normal and anomalous data."
      ],
      "metadata": {
        "id": "37tcf2Z2wOri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 4**"
      ],
      "metadata": {
        "id": "gOC7nkKcwlMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main categories of anomaly detection algorithms are:\n",
        "\n",
        "Statistical methods: These methods use statistical models to identify data points that deviate significantly from the expected or normal behavior. Examples include Gaussian mixture models, kernel density estimation, and statistical hypothesis testing.\n",
        "\n",
        "Machine learning methods: These methods use machine learning algorithms to learn the normal behavior of the data and identify anomalies as data points that are significantly different from the learned behavior. Examples include clustering, decision trees, and neural networks.\n",
        "\n",
        "Information-theoretic methods: These methods use information-theoretic measures such as entropy, mutual information, and divergence to identify anomalies in the data.\n",
        "\n",
        "Time-series methods: These methods analyze time-series data to detect anomalies based on changes in the pattern of the data over time.\n",
        "\n",
        "Spectral methods: These methods analyze the spectral properties of the data to identify anomalies based on their frequency content.\n",
        "\n",
        "Rule-based methods: These methods use predefined rules or heuristics to identify anomalies based on specific domain knowledge or expert input."
      ],
      "metadata": {
        "id": "CviNil6Hwmwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 5**"
      ],
      "metadata": {
        "id": "ZDA4TdfOxNJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Distance-based anomaly detection methods make the following assumptions:\n",
        "\n",
        "Normal data points are clustered together and are similar to each other in terms of their distance from one another.\n",
        "\n",
        "Anomalous data points are far away from the normal data points and are isolated or form small clusters.\n",
        "\n",
        "The distance metric used to calculate the distance between data points is appropriate for the data being analyzed.\n",
        "\n",
        "The distribution of the data points in the feature space is uniform or not skewed towards a particular region.\n",
        "\n",
        "The data points are independent and identically distributed (i.i.d), meaning that each data point is drawn from the same distribution and is not influenced by other data points."
      ],
      "metadata": {
        "id": "_a7P5sB_xPPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 6**"
      ],
      "metadata": {
        "id": "yQ6MOP7nxcJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the local density deviation of a given data point with respect to its k-nearest neighbors. Specifically, the LOF score of a data point x measures how much more or less densely surrounded x is compared to its k-nearest neighbors.\n",
        "\n",
        "To compute the LOF score of a data point x:\n",
        "\n",
        "Find the k-nearest neighbors of x.\n",
        "Compute the reachability distance (reach-dist) of each neighbor of x, defined as the maximum of the distance between x and the neighbor, and the k-distance of the neighbor.\n",
        "Compute the local reachability density (lrd) of x as the inverse of the average reach-dist of its k-nearest neighbors.\n",
        "Compute the LOF score of x as the average lrd of its k-nearest neighbors divided by its own lrd.\n",
        "Intuitively, if the LOF score of a data point x is close to 1, it means that x is similarly dense as its k-nearest neighbors, and hence, is likely a normal data point. Conversely, if the LOF score of x is significantly greater than 1, it means that x is less dense than its neighbors and is therefore likely an outlier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "47CzX7-_xd8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 7**"
      ],
      "metadata": {
        "id": "j3du5QkDyFB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Isolation Forest algorithm has two main parameters:\n",
        "\n",
        "Number of Trees (n_estimators): This parameter specifies the number of trees to be built in the isolation forest. Increasing the number of trees generally improves the quality of anomaly detection but also increases the computational cost.\n",
        "\n",
        "Subsample Size (max_samples): This parameter specifies the size of the subsample to be used to build each tree. A smaller subsample size generally leads to faster model training, but it may also result in a less accurate model."
      ],
      "metadata": {
        "id": "I55KNyICyG06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 8**"
      ],
      "metadata": {
        "id": "3bL1mdI6yYaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The anomaly score of a data point using KNN with K=10 is based on the distance from the point to its 10th nearest neighbor. If a data point has only 2 neighbors of the same class within a radius of 0.5, then its 10th nearest neighbor is likely to be at a relatively large distance. As a result, the anomaly score of the point would be high. However, the exact value of the score would depend on the distances to the other neighbors and the distribution of the distances in the dataset."
      ],
      "metadata": {
        "id": "a4Q4hMvfyb38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques 9**"
      ],
      "metadata": {
        "id": "jtl69IvAytMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Isolation Forest algorithm computes anomaly scores based on the average path length of data points in the trees. The average path length measures the average number of edges that a data point passes through before reaching an isolate node (a terminal node with only one data point) in a tree. The anomaly score is then calculated as the average path length of a data point over all the trees, normalized by a constant value.\n",
        "\n",
        "The formula for the anomaly score in Isolation Forest is given as:\n",
        "\n",
        "anomaly score = 2^(-average path length / c)\n",
        "\n",
        "where c is a normalization factor calculated as c = 2 * (log(n-1) + 0.5772156649), where n is the number of data points in the dataset.\n",
        "\n",
        "Given 100 trees and a dataset of 3000 data points, the normalization factor c can be calculated as:\n",
        "\n",
        "c = 2 * (log(3000-1) + 0.5772156649) = 11.8953\n",
        "\n",
        "If a data point has an average path length of 5.0 compared to the average path length of the trees, its anomaly score can be calculated as:\n",
        "\n",
        "anomaly score = 2^(-5.0/11.8953) = 0.3796\n",
        "\n",
        "Therefore, the anomaly score for this data point is 0.3796."
      ],
      "metadata": {
        "id": "8M6XyfbpyvHe"
      }
    }
  ]
}